# PPO-based 2D Path Planning for UGV

This project implements an autonomous path palnning system for an Unmanned Ground Vehicle (UGV) using the Proximal Policy Optimization (PPO) reinforcement learning algorithm. The UGV learns to navigate from a start point to a moving target point in a continuous 2D environment.

## üöó Overview

The task is framed as a reinforcement learning problem where the agent (UGV) must learn an optimal policy to reach the target efficiently and safely. The state and action spaces are continuous, and the agent receives rewards based on its distance to the target.

## üìå Problem Setup

- **Agent**: UGV (Ackermann kinematic model)
- **Environment**: 2D continuous space with moving target
- **Objective**: Reach the target location from a given start point 
- **Algorithm**: Proximal Policy Optimization (PPO)

## üß† Observation Space

The observation space of the system is represented by the UGV's position (X, Y), its orientation ($\theta$) and the target position ($$\ X_t \$$ , $$\ Y_t \$$).
  - $X, Y$ : UGV Position on a 2D grid.
  - $\theta$ : UGV Orientation (direction) of the UGV, with possible values: [0, 90, 180, 270] degrees.
  - $$\ X_t \$$ , $$\ Y_t \$$ : Target Position

  ```python
# State space bounds
self.x_min, self.x_max = -2, 2
self.y_min, self.y_max = -2, 2
self.theta_min, self.theta_max = np.deg2rad(-180), np.deg2rad(180)

# Observation space: [x, y, theta, x_target, y_target]
self.observation_space = spaces.Box(
            low=np.array([self.x_min, self.y_min, self.theta_min, self.x_min, self.y_min]),
            high=np.array([self.x_max, self.y_max, self.theta_max, self.x_max, self.y_max]),
            dtype=np.float32
        )
```

## üéÆ Action Space

The action space consists of two continuous control inputs:

- **Steering angle** (in radians)
- **Velocity** (in meters/second)

The **neural network (policy)** outputs actions in the normalized range:
- Steering angle ‚àà **[-1, 1]**
- Velocity ‚àà **[0, 1]**

```python
self.action_space = spaces.Box(
    low=np.array([-1, 0]),
    high=np.array([1, 1]),
    dtype=np.float32
)
```

## üìê Control Bounds

These normalized outputs from the policy network are scaled to the actual control limits before being applied in the environment.

```python
self.v_max = 0.6
self.v_min = 0.0

self.steer_max = np.deg2rad(25)   # ‚âà 0.4363 rad
self.steer_min = np.deg2rad(-25)  # ‚âà -0.4363 rad
```

## üîÑ Action Scaling Function

To convert normalized actions to their real-world equivalents, the following function is used:

```python
def scale_action(self, action):
    """Scales action from [-1,1] and [0,1] to real-world bounds."""
    steer_angle = self.steer_min + (action[0] + 1) * 0.5 * (self.steer_max - self.steer_min)
    v = self.v_min + (action[1] + 1) * 0.5 * (self.v_max - self.v_min)
    return steer_angle, v

def get_next_observation(self, observation, action):    
    steer_angle, v = self.scale_action(action)
    ...
```

## üßÆ Scaling Equations

Let the normalized action from the policy be:

- $\ a_0$ in [-1, 1] ‚Äî **Steering angle** ( $\delta \$)
- $\ a_1$ in [0, 1] ‚Äî **Velocity** $\( v $\)

Then the scaled real-world values are computed as:

### ‚úÖ Steering Angle ( $\delta \$)
$$
\
\delta = \delta_{\text{min}} + \frac{(a_0 + 1)}{2} \cdot (\delta_{\text{max}} - \delta_{\text{min}})
\
$$

### ‚úÖ Velocity
$$
\
v = v_{\text{min}} + \frac{(a_1 + 1)}{2} \cdot (v_{\text{max}} - v_{\text{min}})
\
$$

This ensures that the policy outputs remain bounded while allowing fine control over the UGV in the continuous environment.

## üèÅ Reward Function

The reward function is designed to encourage the UGV to:

- Move closer to the target
- Avoid collisions
- Minimize time taken to reach the target
- Penalize erratic or unsafe maneuvers

## ‚öôÔ∏è Requirements

- Python 3.x
- PyTorch
- NumPy
- Matplotlib
- Gym (custom environment)
- [Optional] PyBullet (if simulation is physics-based)

Install dependencies:

```bash
pip install -r requirements.txt

